{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d77754-2efe-446c-9e4c-7798b9d52c77",
   "metadata": {},
   "source": [
    "# Acme Corp: Transforming Customer Feedback into Strategic Insights\n",
    "## Business Problem Scenario\n",
    "In today's competitive e-commerce environment, understanding customer sentiment is more crucial than ever. With the rise of online reviews, social media feedback, and customer interactions, businesses are inundated with data that can significantly impact their strategies. However, manually sifting through this information can be overwhelming and inefficient.\n",
    "\n",
    "To tackle this challenge, Acme Corp, a leader in the online retail sector, recognizes the need for an innovative solution to streamline sentiment analysis. By leveraging advanced artificial intelligence techniques like Natural Language processing (NLP), the company aims to automate the process of analyzing customer reviews and transforming raw feedback into valuable insight by leveraging the power of transformers.\n",
    "\n",
    "This AI model will provide significant support in:\n",
    "- Real-time Insights: Gain immediate understanding of customer sentiment, enabling proactive responses to feedback.\n",
    "- Data-Driven Decisions: Inform product development and marketing strategies based on comprehensive analysis of customer opinions.\n",
    "- Enhanced Customer Engagement: Identify and address negative sentiments quickly, improving overall customer satisfaction and loyalty.\n",
    "- Operational Efficiency: Reduce the manual effort required for sentiment analysis, allowing team members to focus on higher-value tasks.\n",
    "\n",
    "Through this project, Acme Corp will harness the power of AI to turn customer feedback into actionable insights, driving business growth and improving customer experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d359cec7-d0d1-4245-8eda-7c7926a742bf",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "435a01d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset , DatasetDict\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup , DistilBertTokenizer, DistilBertModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import optuna\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63a6653-9db6-44e9-a90a-4188fab9b1ce",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370b85f-4df6-40b7-98ab-f89604537647",
   "metadata": {},
   "source": [
    "we will proceed with these steps to load our data successfully  :\n",
    "- load the Stanford SST2 dataset from Hugging face ,\n",
    "- remove the unnecessary idx column\n",
    "- check the dataset size to validate the idx column removal\n",
    "- The data is already classified to training , validating and testing , we just need to assign each of them to a **pandas** data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05f0fc9e-cec8-43b8-9e33-e5c382784e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\"glue\", \"sst2\") #loading dataset\n",
    "\n",
    "# Define a function to remove the idx column from a dataset\n",
    "def remove_column(dataset):\n",
    "    return dataset.remove_columns('idx')\n",
    "\n",
    "# Apply the function to each dataset in the DatasetDict\n",
    "df = DatasetDict({\n",
    "    split: remove_column(dataset)\n",
    "    for split, dataset in df.items()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "927c52dd-707d-4e15-8ce2-66f42465f43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (67349, 2), 'validation': (872, 2), 'test': (1821, 2)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape #checking dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67fc52fe-1804-4744-b23e-b69e299cd031",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(df['train'])\n",
    "df_valid = pd.DataFrame(df['validation'])\n",
    "df_test = pd.DataFrame(df['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa99f54-cea3-42d6-a7de-b61b2b520696",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b265527c-190d-4b64-84a0-847f8f958b21",
   "metadata": {},
   "source": [
    "We will define a custom dataset class to handle tokenization and prepare our input data for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbfa7cad-89ec-4167-be49-3f3c7e26457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SST2Dataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'sentence': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased' ,  clean_up_tokenization_spaces=True)\n",
    "max_length = 128\n",
    "\n",
    "train_dataset = SST2Dataset(df_train['sentence'], df_train['label'], tokenizer, max_length)\n",
    "val_dataset = SST2Dataset(df_valid['sentence'], df_valid['label'], tokenizer, max_length)\n",
    "test_dataset = SST2Dataset(df_test['sentence'], df_test['label'], tokenizer, max_length)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True )\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b401860d-2139-4fd0-b5c7-30145c3a59ae",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f988b59-bb26-4935-bbc1-84de01e9e0d2",
   "metadata": {},
   "source": [
    "We will create the sentiment classification model using  DistilBERT pretrained architecture , we will define it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04b0cab0-ff18-40cf-8483-6d5f256dd63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, model,  dropout_rate, n_classes=2,):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.model = model\n",
    "        self.drop = nn.Dropout(dropout_rate)\n",
    "        self.out = nn.Linear(self.model.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get the last hidden state from DistilBERT\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state  # [batch_size, sequence_length, hidden_size]\n",
    "        \n",
    "        # Use the hidden state of the first token ([CLS] token)\n",
    "        cls_output = hidden_state[:, 0, :]  # [batch_size, hidden_size]\n",
    "        output = self.drop(cls_output)\n",
    "        return self.out(output)\n",
    "\n",
    "        \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e462c4-6e65-4302-875d-289d546f8f01",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "218d5b9c-7db5-4f49-9ba6-6244eac5e0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss Function\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f660cf11-e8b4-4133-b70c-808c5ff4a20b",
   "metadata": {},
   "source": [
    "## Model Training and Hyper parameter tuning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6ef186a-3d9b-4075-a07a-f4360e9a90bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypertune(model, optimizer, train_loader, valid_loader, criterion, device, trial):\n",
    "    # Training phase\n",
    "    for epoch in range(3):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "\n",
    "        # Training loop\n",
    "        for batch in tqdm(train_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            train_correct += torch.sum(predicted == labels).item()\n",
    "        \n",
    "        # Compute average training loss and accuracy\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = train_correct / len(train_loader.dataset)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        valid_correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                valid_correct += torch.sum(predicted == labels).item()\n",
    "\n",
    "        valid_loss /= len(valid_loader)\n",
    "        valid_accuracy = valid_correct / len(valid_loader.dataset)\n",
    "\n",
    "        trial.report(valid_loss , epoch)\n",
    "\n",
    "        # Prune the trial if Optuna decides it's not worth continuing\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # Return final validation accuracy and loss (to be used in objective function)\n",
    "    return  valid_loss\n",
    "\n",
    "\n",
    "\n",
    "def model_train(model, optimizer, train_loader, val_loader, criterion, device):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Adjust the learning rate\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            train_correct += torch.sum(predicted == labels).item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = train_correct / len(train_loader.dataset)\n",
    "\n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                val_correct += torch.sum(predicted == labels).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = val_correct / len(val_loader.dataset)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '\n",
    "              f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "        return train_accuracy, val_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3663ad-1f47-4355-9706-8703d7f560e8",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec59686-e7d9-49b8-a79f-c2d4a70989d8",
   "metadata": {},
   "source": [
    "We will use **optuna** framework to tune our hyperparameters which are  : \n",
    "- learning rate \n",
    "- dropout rete \n",
    "- and weight decay\n",
    "\n",
    "we will define the **model** now and the **optimizer** to use them in the tuning process.\n",
    "\n",
    "model --> as mentioned , we are using distilbert model , **DistilBERT** is a smaller, faster, and more efficient version of BERT (Bidirectional Encoder Representations from Transformers). It retains about 97% of BERT's language understanding capabilities while being 60% smaller and 2x faster. This makes it a great option for tasks like sentiment analysis where performance and speed are important.\n",
    "\n",
    "optimizer--> we are using **AdamW** optimizer ,  The **AdamW** optimizer is an extension of the Adam optimizer. It decouples the weight decay (L2 regularization) from the gradient update to perform better with large models like BERT or DistilBERT. In the context of transformers, **AdamW** is commonly used because it improves the convergence of the model while preventing overfitting by adding weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4d3cc70-5471-4f21-9519-2179671ef135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-06 19:38:24,337] A new study created in memory with name: no-name-e6a464c2-700c-4bd9-b8ca-b4957aa1fadc\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [07:05<00:00,  4.94it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [41:57<00:00,  1.20s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:28<00:00,  5.42it/s]\n",
      "[I 2024-10-06 20:34:02,185] Trial 0 finished with value: 0.7142463390316282 and parameters: {'learning_rate': 0.004740262844292552, 'weight_decay': 0.008056728610755317, 'dropout_rate': 0.3781735694792884}. Best is trial 0 with value: 0.7142463390316282.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:30<00:00,  5.39it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:28<00:00,  5.42it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:28<00:00,  5.42it/s]\n",
      "[I 2024-10-06 20:53:35,431] Trial 1 finished with value: 0.6950094721146992 and parameters: {'learning_rate': 0.0003928812276675629, 'weight_decay': 0.003916506543784848, 'dropout_rate': 0.37126414312163747}. Best is trial 1 with value: 0.6950094721146992.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:42<00:00,  5.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:28<00:00,  5.42it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:36<00:00,  5.31it/s]\n",
      "[I 2024-10-06 21:13:30,142] Trial 2 finished with value: 0.6958375445434025 and parameters: {'learning_rate': 0.0010838081162146968, 'weight_decay': 0.0035627795171312504, 'dropout_rate': 0.47516907285443244}. Best is trial 1 with value: 0.6950094721146992.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:35<00:00,  5.32it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:35<00:00,  5.32it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:36<00:00,  5.31it/s]\n",
      "[I 2024-10-06 21:33:23,891] Trial 3 finished with value: 0.7014258291040149 and parameters: {'learning_rate': 0.0016838152625784969, 'weight_decay': 0.0004291572311774543, 'dropout_rate': 0.39546907472036097}. Best is trial 1 with value: 0.6950094721146992.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:34<00:00,  5.34it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.37it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "[I 2024-10-06 21:53:09,240] Trial 4 finished with value: 0.7070765665599278 and parameters: {'learning_rate': 0.0027117802437164147, 'weight_decay': 0.0011892594654532222, 'dropout_rate': 0.40387101196056496}. Best is trial 1 with value: 0.6950094721146992.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.37it/s]\n",
      "[I 2024-10-06 21:59:44,585] Trial 5 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:31<00:00,  5.37it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:48<00:00,  5.15it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [07:30<00:00,  4.68it/s]\n",
      "[I 2024-10-06 22:20:42,039] Trial 6 finished with value: 0.7026152908802032 and parameters: {'learning_rate': 0.00264868266274964, 'weight_decay': 0.005394828849615264, 'dropout_rate': 0.5141838560683438}. Best is trial 1 with value: 0.6950094721146992.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:37<00:00,  5.30it/s]\n",
      "[I 2024-10-06 22:27:21,700] Trial 7 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:31<00:00,  5.38it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:31<00:00,  5.38it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [07:12<00:00,  4.86it/s]\n",
      "[I 2024-10-06 22:47:43,952] Trial 8 finished with value: 0.693579090493066 and parameters: {'learning_rate': 0.004303631993789388, 'weight_decay': 0.006168924185653822, 'dropout_rate': 0.5706090471527338}. Best is trial 8 with value: 0.693579090493066.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [08:29<00:00,  4.13it/s]\n",
      "[I 2024-10-06 22:56:18,022] Trial 9 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:52<00:00,  5.10it/s]\n",
      "[I 2024-10-06 23:03:12,976] Trial 10 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:38<00:00,  5.29it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:35<00:00,  5.32it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:40<00:00,  5.25it/s]\n",
      "[I 2024-10-06 23:23:15,363] Trial 11 finished with value: 0.6971498855522701 and parameters: {'learning_rate': 0.0002265529612151211, 'weight_decay': 0.002964605426716708, 'dropout_rate': 0.2900662961911209}. Best is trial 8 with value: 0.693579090493066.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:40<00:00,  5.25it/s]\n",
      "[I 2024-10-06 23:29:58,282] Trial 12 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:35<00:00,  5.32it/s]\n",
      "[I 2024-10-06 23:36:36,547] Trial 13 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "[I 2024-10-06 23:56:20,458] Trial 14 finished with value: 0.6952112168073654 and parameters: {'learning_rate': 0.0036655809431397107, 'weight_decay': 0.006042700339933989, 'dropout_rate': 0.6987327261651604}. Best is trial 8 with value: 0.693579090493066.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:34<00:00,  5.34it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:33<00:00,  5.34it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:31<00:00,  5.38it/s]\n",
      "[I 2024-10-07 00:16:06,266] Trial 15 finished with value: 0.6971939227410725 and parameters: {'learning_rate': 0.004242227957528728, 'weight_decay': 0.004699759989477402, 'dropout_rate': 0.4573465046238642}. Best is trial 8 with value: 0.693579090493066.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:31<00:00,  5.37it/s]\n",
      "[I 2024-10-07 00:22:41,637] Trial 16 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.37it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:31<00:00,  5.37it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:31<00:00,  5.37it/s]\n",
      "[I 2024-10-07 00:42:23,344] Trial 17 finished with value: 0.2617493286462767 and parameters: {'learning_rate': 1.9017935651227365e-05, 'weight_decay': 0.006633894347634678, 'dropout_rate': 0.34774534977754684}. Best is trial 17 with value: 0.2617493286462767.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:35<00:00,  5.33it/s]\n",
      "[I 2024-10-07 00:49:01,217] Trial 18 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:35<00:00,  5.32it/s]\n",
      "[I 2024-10-07 00:55:39,553] Trial 19 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:37<00:00,  5.30it/s]\n",
      "[I 2024-10-07 01:02:19,625] Trial 20 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:39<00:00,  5.27it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "[I 2024-10-07 01:22:10,336] Trial 21 finished with value: 0.4724685102701187 and parameters: {'learning_rate': 0.00016662675855287038, 'weight_decay': 0.005139576223791274, 'dropout_rate': 0.3645104094337399}. Best is trial 17 with value: 0.2617493286462767.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "[I 2024-10-07 01:41:54,634] Trial 22 finished with value: 0.36504499348146574 and parameters: {'learning_rate': 8.062128112157427e-05, 'weight_decay': 0.00879315556523614, 'dropout_rate': 0.4220391714779153}. Best is trial 17 with value: 0.2617493286462767.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:38<00:00,  5.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [08:30<00:00,  4.12it/s]\n",
      "[I 2024-10-07 02:03:44,222] Trial 23 finished with value: 0.49817452925656525 and parameters: {'learning_rate': 0.00013542891066583969, 'weight_decay': 0.008861447798275461, 'dropout_rate': 0.44086921830623105}. Best is trial 17 with value: 0.2617493286462767.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [08:29<00:00,  4.13it/s]\n",
      "[I 2024-10-07 02:12:16,286] Trial 24 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [08:30<00:00,  4.12it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [07:36<00:00,  4.61it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:34<00:00,  5.34it/s]\n",
      "[I 2024-10-07 02:35:04,251] Trial 25 finished with value: 0.3135741378313729 and parameters: {'learning_rate': 7.008538444608485e-05, 'weight_decay': 0.007044409140457758, 'dropout_rate': 0.2789639514360567}. Best is trial 17 with value: 0.2617493286462767.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "[I 2024-10-07 02:41:39,775] Trial 26 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "[I 2024-10-07 02:48:15,172] Trial 27 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:34<00:00,  5.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:35<00:00,  5.32it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:33<00:00,  5.34it/s]\n",
      "[I 2024-10-07 03:08:05,287] Trial 28 finished with value: 0.41144515361104694 and parameters: {'learning_rate': 8.17513360851152e-05, 'weight_decay': 0.007970698107828422, 'dropout_rate': 0.4237588821676894}. Best is trial 17 with value: 0.2617493286462767.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "[I 2024-10-07 03:14:41,025] Trial 29 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "[I 2024-10-07 03:21:16,261] Trial 30 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:33<00:00,  5.36it/s]\n",
      "[I 2024-10-07 03:41:00,903] Trial 31 finished with value: 0.3958598596176931 and parameters: {'learning_rate': 9.075659689310925e-05, 'weight_decay': 0.007933409594769812, 'dropout_rate': 0.42429001487655155}. Best is trial 17 with value: 0.2617493286462767.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "[I 2024-10-07 03:47:35,642] Trial 32 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:34<00:00,  5.34it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "[I 2024-10-07 04:07:21,616] Trial 33 finished with value: 0.297226951324514 and parameters: {'learning_rate': 2.34907244256776e-05, 'weight_decay': 0.00771634731663679, 'dropout_rate': 0.3009305451588299}. Best is trial 17 with value: 0.2617493286462767.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "[I 2024-10-07 04:13:56,690] Trial 34 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "[I 2024-10-07 04:20:31,860] Trial 35 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "[I 2024-10-07 04:27:08,156] Trial 36 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:33<00:00,  5.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "[I 2024-10-07 04:46:53,249] Trial 37 finished with value: 0.2987286377964275 and parameters: {'learning_rate': 2.75737641550623e-05, 'weight_decay': 0.005748457130866672, 'dropout_rate': 0.30671784102484023}. Best is trial 17 with value: 0.2617493286462767.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "[I 2024-10-07 04:53:28,234] Trial 38 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "[I 2024-10-07 05:00:03,802] Trial 39 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "[I 2024-10-07 05:06:38,713] Trial 40 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "[I 2024-10-07 05:13:13,824] Trial 41 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:33<00:00,  5.35it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "[I 2024-10-07 05:32:58,625] Trial 42 finished with value: 0.3553984287594046 and parameters: {'learning_rate': 1.2755391857069417e-05, 'weight_decay': 0.008279262329483797, 'dropout_rate': 0.283266023857077}. Best is trial 17 with value: 0.2617493286462767.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "[I 2024-10-07 05:52:43,330] Trial 43 finished with value: 0.27835758881909506 and parameters: {'learning_rate': 1.0669561680888868e-05, 'weight_decay': 0.008302178055511965, 'dropout_rate': 0.2842034777887744}. Best is trial 17 with value: 0.2617493286462767.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "[I 2024-10-07 05:59:19,511] Trial 44 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "[I 2024-10-07 06:05:54,425] Trial 45 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n",
      "[I 2024-10-07 06:12:29,716] Trial 46 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:34<00:00,  5.33it/s]\n",
      "[I 2024-10-07 06:19:06,779] Trial 47 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:33<00:00,  5.34it/s]\n",
      "[I 2024-10-07 06:25:43,017] Trial 48 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:34<00:00,  5.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:34<00:00,  5.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:34<00:00,  5.34it/s]\n",
      "[I 2024-10-07 06:45:33,352] Trial 49 finished with value: 0.37566009376730236 and parameters: {'learning_rate': 3.0312296230265466e-05, 'weight_decay': 0.00945865105129148, 'dropout_rate': 0.262280381965978}. Best is trial 17 with value: 0.2617493286462767.\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    learningrate = trial.suggest_float('learning_rate', 1e-5, 5e-3)\n",
    "    weightdecay = trial.suggest_float('weight_decay', 1e-4, 1e-2)\n",
    "    dropoutrate = trial.suggest_float('dropout_rate', 0.2, 0.7)\n",
    "\n",
    "    bert_model_tuning = SentimentClassifier(model = DistilBertModel.from_pretrained('distilbert-base-uncased') , dropout_rate = dropoutrate )\n",
    "\n",
    "    model_tuning = bert_model_tuning.to(device)\n",
    "\n",
    "    optimizer_tuning = torch.optim.AdamW(model_tuning.parameters(), lr=learningrate, weight_decay=weightdecay)\n",
    "\n",
    "    \n",
    "\n",
    "    valid_loss = hypertune(\n",
    "        model_tuning,\n",
    "        optimizer_tuning,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        device,\n",
    "        trial\n",
    "    )\n",
    "    \n",
    "    return valid_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best_trial = study.best_trial\n",
    "learning_rate = best_trial.params['learning_rate']\n",
    "weight_decay = best_trial.params['weight_decay']\n",
    "dropout_rate = best_trial.params['dropout_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11101217-1f81-4b2e-98c3-e39b7747be96",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee14af57-d587-4619-aa43-722dd0cd6e8d",
   "metadata": {},
   "source": [
    "now we will use the best hyperparameters with our model and optimizer to proceed with the model training.\n",
    "\n",
    "we will implement **get_linear_schedule_with_warmup** scheduler , which is important for stabilizing training, especially when fine-tuning large pre-trained models like **DistilBERT**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffe5ee5c-afb2-400f-99d6-78496ea819f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_tuned = SentimentClassifier(model = DistilBertModel.from_pretrained('distilbert-base-uncased') , dropout_rate = dropout_rate )\n",
    "\n",
    "model_tuned = bert_model_tuned.to(device)\n",
    "\n",
    "optimizer_tuned = torch.optim.AdamW(model_tuned.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "warmup_steps = int(0.1 * total_steps)  # Typically 10% of the total steps\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer_tuned,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120d5ad5-69c8-45d7-b0a8-c557bfa118d8",
   "metadata": {},
   "source": [
    "We will train with only 3 epochs , various experiments were conducted with more epochs but it ended up with the model to overfit , so it was observed that more than 2 epochs cause overfitting , this is because it's a pretrained model , 2-5 epochs will be enough , according to different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d70e0d1-6655-42e7-9915-b3471a2ab4fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Training Epoch 1 /3 ------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:32<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Train Loss: 0.2598, Train Accuracy: 0.8852, Validation Loss: 0.2360, Validation Accuracy: 0.9014\n",
      "------------------ Training Epoch 2 /3 ------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:31<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Train Loss: 0.1181, Train Accuracy: 0.9584, Validation Loss: 0.2493, Validation Accuracy: 0.9140\n",
      "------------------ Training Epoch 3 /3 ------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:31<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Train Loss: 0.0760, Train Accuracy: 0.9734, Validation Loss: 0.2640, Validation Accuracy: 0.9128\n",
      "Total training time: 0.33 hours\n"
     ]
    }
   ],
   "source": [
    "training_start_time = time.time()\n",
    "\n",
    "#This code is for implementing early stopping , it's useless here since we are working only with 3 epochs\n",
    "# patience = 15 # this variable is to apply early stopping technique\n",
    "# best_val_accuracy = 0.0\n",
    "# epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"------------------ Training Epoch {epoch+1} /{num_epochs} ------------------\")\n",
    "    train_accuracy, val_accuracy = model_train(model_tuned, optimizer_tuned, train_loader, val_loader, criterion, device)\n",
    "    \n",
    "    #If statement for early stopping mechanism\n",
    "    # if val_accuracy > best_val_accuracy:\n",
    "    #     best_val_accuracy = val_accuracy\n",
    "    #     epochs_without_improvement = 0\n",
    "    # else:\n",
    "    #     epochs_without_improvement += 1\n",
    "\n",
    "    # if epochs_without_improvement >= patience:\n",
    "    #     print(\"Early stopping triggered\")\n",
    "    #     break\n",
    "    \n",
    "training_end_time = time.time()\n",
    "\n",
    "# Calculate the total training time in hours\n",
    "total_training_time = (training_end_time - training_start_time) /3600\n",
    "print(f\"Total training time: {total_training_time:.2f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7ea05e-ab21-413d-b33c-0e5599571e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d59f0ca2-8ff5-4b5c-8458-cb02899c3488",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SentimentClassifier' object has no attribute 'save_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model_tuned\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-sentiment-model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Save the tokenizer\u001b[39;00m\n\u001b[0;32m      5\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-sentiment-model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SentimentClassifier' object has no attribute 'save_pretrained'"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model_tuned.save_pretrained(\"distilbert-sentiment-model\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(\"distilbert-sentiment-model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57009013-4e05-4c46-adf9-b12e1b80686f",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475ed7d2-4f54-439e-aae8-25718862384e",
   "metadata": {},
   "source": [
    "In this notebook, we developed an advanced sentiment analysis model utilizing the DistilBERT architecture, fine-tuning it to accurately classify sentiments from textual data. We employed get_linear_schedule_with_warmup learning rate scheduler to optimize the training process. The model was rigorously evaluated on distinct training, validation, and test sets to ensure reliable accuracy and generalization.\n",
    "\n",
    "Additionally, we prepared the model for deployment on Hugging Face (deployment is in a seperate notebook), allowing for easy testing of individual text inputs to predict sentiment classifications. \n",
    "\n",
    "This project serves as a comprehensive and practical tool for sentiment analysis, providing valuable insights for understanding public opinions and sentiments in various applications.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
