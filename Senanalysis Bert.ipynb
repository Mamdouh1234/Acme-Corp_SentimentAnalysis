{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d77754-2efe-446c-9e4c-7798b9d52c77",
   "metadata": {},
   "source": [
    "# Acme Corp: Transforming Customer Feedback into Strategic Insights\n",
    "## Business Problem Scenario\n",
    "In today's competitive e-commerce environment, understanding customer sentiment is more crucial than ever. With the rise of online reviews, social media feedback, and customer interactions, businesses are inundated with data that can significantly impact their strategies. However, manually sifting through this information can be overwhelming and inefficient.\n",
    "\n",
    "To tackle this challenge, Acme Corp, a leader in the online retail sector, recognizes the need for an innovative solution to streamline sentiment analysis. By leveraging advanced artificial intelligence techniques like Natural Language processing (NLP), the company aims to automate the process of analyzing customer reviews and transforming raw feedback into valuable insight by leveraging the power of transformers.\n",
    "\n",
    "This AI model will provide significant support in:\n",
    "- Real-time Insights: Gain immediate understanding of customer sentiment, enabling proactive responses to feedback.\n",
    "- Data-Driven Decisions: Inform product development and marketing strategies based on comprehensive analysis of customer opinions.\n",
    "- Enhanced Customer Engagement: Identify and address negative sentiments quickly, improving overall customer satisfaction and loyalty.\n",
    "- Operational Efficiency: Reduce the manual effort required for sentiment analysis, allowing team members to focus on higher-value tasks.\n",
    "\n",
    "Through this project, Acme Corp will harness the power of AI to turn customer feedback into actionable insights, driving business growth and improving customer experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d359cec7-d0d1-4245-8eda-7c7926a742bf",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "435a01d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset , DatasetDict\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup , DistilBertTokenizer, DistilBertModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import optuna\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63a6653-9db6-44e9-a90a-4188fab9b1ce",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370b85f-4df6-40b7-98ab-f89604537647",
   "metadata": {},
   "source": [
    "we will proceed with these steps to load our data successfully  :\n",
    "- load the Stanford SST2 dataset from Hugging face ,\n",
    "- remove the unnecessary idx column\n",
    "- check the dataset size to validate the idx column removal\n",
    "- The data is already classified to training , validating and testing , we just need to assign each of them to a **pandas** data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05f0fc9e-cec8-43b8-9e33-e5c382784e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "# Define a function to remove the idx column from a dataset\n",
    "def remove_column(dataset):\n",
    "    return dataset.remove_columns('idx')\n",
    "\n",
    "# Apply the function to each dataset in the DatasetDict\n",
    "df = DatasetDict({\n",
    "    split: remove_column(dataset)\n",
    "    for split, dataset in df.items()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "927c52dd-707d-4e15-8ce2-66f42465f43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (67349, 2), 'validation': (872, 2), 'test': (1821, 2)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67fc52fe-1804-4744-b23e-b69e299cd031",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(df['train'])\n",
    "df_valid = pd.DataFrame(df['validation'])\n",
    "df_test = pd.DataFrame(df['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa99f54-cea3-42d6-a7de-b61b2b520696",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b265527c-190d-4b64-84a0-847f8f958b21",
   "metadata": {},
   "source": [
    "We will define a custom dataset class to handle tokenization and prepare our input data for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbfa7cad-89ec-4167-be49-3f3c7e26457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SST2Dataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'sentence': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased' ,  clean_up_tokenization_spaces=True)\n",
    "max_length = 128\n",
    "\n",
    "train_dataset = SST2Dataset(df_train['sentence'], df_train['label'], tokenizer, max_length)\n",
    "val_dataset = SST2Dataset(df_valid['sentence'], df_valid['label'], tokenizer, max_length)\n",
    "test_dataset = SST2Dataset(df_test['sentence'], df_test['label'], tokenizer, max_length)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True )\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b401860d-2139-4fd0-b5c7-30145c3a59ae",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f988b59-bb26-4935-bbc1-84de01e9e0d2",
   "metadata": {},
   "source": [
    "We will create the sentiment classification model using  DistilBERT pretrained architecture , we will define it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04b0cab0-ff18-40cf-8483-6d5f256dd63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, model,  dropout_rate, n_classes=2,):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.model = model\n",
    "        self.drop = nn.Dropout(dropout_rate)\n",
    "        self.out = nn.Linear(self.model.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get the last hidden state from DistilBERT\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state  # [batch_size, sequence_length, hidden_size]\n",
    "        \n",
    "        # Use the hidden state of the first token ([CLS] token)\n",
    "        cls_output = hidden_state[:, 0, :]  # [batch_size, hidden_size]\n",
    "        output = self.drop(cls_output)\n",
    "        return self.out(output)\n",
    "\n",
    "        \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e462c4-6e65-4302-875d-289d546f8f01",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "218d5b9c-7db5-4f49-9ba6-6244eac5e0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss Function\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f660cf11-e8b4-4133-b70c-808c5ff4a20b",
   "metadata": {},
   "source": [
    "## Model Training and Hyper parameter tuning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6ef186a-3d9b-4075-a07a-f4360e9a90bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypertune(model, optimizer, train_loader, criterion, device , trial):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader)):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            train_correct += torch.sum(predicted == labels).item()\n",
    "            \n",
    "            # Report intermediate results to Optuna\n",
    "            intermediate_accuracy = train_correct / len(train_loader.dataset)\n",
    "\n",
    "            if batch_idx % 20 == 0:\n",
    "                trial.report(1 - intermediate_accuracy, batch_idx)\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    \n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = train_correct / len(train_loader.dataset)\n",
    "\n",
    "        return train_accuracy\n",
    "\n",
    "\n",
    "\n",
    "def model_train(model, optimizer, train_loader, val_loader, criterion, device):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Adjust the learning rate\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            train_correct += torch.sum(predicted == labels).item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = train_correct / len(train_loader.dataset)\n",
    "\n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                val_correct += torch.sum(predicted == labels).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = val_correct / len(val_loader.dataset)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '\n",
    "              f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "        return train_accuracy, val_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3663ad-1f47-4355-9706-8703d7f560e8",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec59686-e7d9-49b8-a79f-c2d4a70989d8",
   "metadata": {},
   "source": [
    "We will use **optuna** framework to tune our hyperparameters which are  : \n",
    "- learning rate \n",
    "- dropout rete \n",
    "- and weight decay\n",
    "\n",
    "we will define the **model** now and the **optimizer** to use them in the tuning process.\n",
    "\n",
    "model --> as mentioned , we are using distilbert model , **DistilBERT** is a smaller, faster, and more efficient version of BERT (Bidirectional Encoder Representations from Transformers). It retains about 97% of BERT's language understanding capabilities while being 60% smaller and 2x faster. This makes it a great option for tasks like sentiment analysis where performance and speed are important.\n",
    "\n",
    "optimizer--> we are using **AdamW** optimizer ,  The **AdamW** optimizer is an extension of the Adam optimizer. It decouples the weight decay (L2 regularization) from the gradient update to perform better with large models like BERT or DistilBERT. In the context of transformers, **AdamW** is commonly used because it improves the convergence of the model while preventing overfitting by adding weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4d3cc70-5471-4f21-9519-2179671ef135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-28 10:23:02,808] A new study created in memory with name: no-name-f4e3ff5b-d7a4-4913-80af-fabca1bfdb6e\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:34<00:00,  5.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:35<00:00,  5.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:33<00:00,  5.35it/s]\n",
      "[I 2024-09-28 10:42:47,172] Trial 0 finished with value: 0.05493771251243518 and parameters: {'learning_rate': 0.00017133144290496324, 'weight_decay': 0.004414139811726712, 'dropout_rate': 0.4806636668253681}. Best is trial 0 with value: 0.05493771251243518.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:33<00:00,  5.35it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:37<00:00,  5.30it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:36<00:00,  5.31it/s]\n",
      "[I 2024-09-28 11:02:35,359] Trial 1 finished with value: 0.45550787688013183 and parameters: {'learning_rate': 0.0024215285097052997, 'weight_decay': 0.0031918044574804193, 'dropout_rate': 0.5610164850635238}. Best is trial 0 with value: 0.05493771251243518.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:36<00:00,  5.30it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:35<00:00,  5.32it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:38<00:00,  5.29it/s]\n",
      "[I 2024-09-28 11:22:26,958] Trial 2 finished with value: 0.45626512643097894 and parameters: {'learning_rate': 0.004017523433846097, 'weight_decay': 0.007695360356231173, 'dropout_rate': 0.5393824620328878}. Best is trial 0 with value: 0.05493771251243518.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:35<00:00,  5.32it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:35<00:00,  5.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:37<00:00,  5.30it/s]\n",
      "[I 2024-09-28 11:42:16,054] Trial 3 finished with value: 0.44927170410844997 and parameters: {'learning_rate': 0.002195224435627003, 'weight_decay': 0.0031495183301062267, 'dropout_rate': 0.4247637394202627}. Best is trial 0 with value: 0.05493771251243518.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:36<00:00,  5.31it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:35<00:00,  5.32it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:34<00:00,  5.33it/s]\n",
      "[I 2024-09-28 12:02:04,012] Trial 4 finished with value: 0.45201858973407183 and parameters: {'learning_rate': 0.0029327830476241915, 'weight_decay': 0.0034454172603535864, 'dropout_rate': 0.3563693639617961}. Best is trial 0 with value: 0.05493771251243518.\n",
      "  1%|▊                                                                               | 20/2105 [00:03<06:49,  5.09it/s]\n",
      "[I 2024-09-28 12:02:08,841] Trial 5 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:33<00:00,  5.35it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:33<00:00,  5.35it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:33<00:00,  5.35it/s]\n",
      "[I 2024-09-28 12:21:50,286] Trial 6 finished with value: 0.44548545635421466 and parameters: {'learning_rate': 0.0015035443800063243, 'weight_decay': 0.00607956310171287, 'dropout_rate': 0.2821617329128656}. Best is trial 0 with value: 0.05493771251243518.\n",
      "  0%|                                                                                         | 0/2105 [00:00<?, ?it/s]\n",
      "[I 2024-09-28 12:21:51,064] Trial 7 pruned. \n",
      "  4%|███                                                                             | 80/2105 [00:15<06:24,  5.27it/s]\n",
      "[I 2024-09-28 12:22:06,756] Trial 8 pruned. \n",
      "  0%|                                                                                         | 0/2105 [00:00<?, ?it/s]\n",
      "[I 2024-09-28 12:22:07,414] Trial 9 pruned. \n",
      " 24%|██████████████████▊                                                            | 500/2105 [01:36<05:09,  5.18it/s]\n",
      "[I 2024-09-28 12:23:44,321] Trial 10 pruned. \n",
      "  0%|                                                                                         | 0/2105 [00:00<?, ?it/s]\n",
      "[I 2024-09-28 12:23:44,975] Trial 11 pruned. \n",
      "  1%|▊                                                                               | 20/2105 [00:03<06:50,  5.08it/s]\n",
      "[I 2024-09-28 12:23:49,403] Trial 12 pruned. \n",
      "  2%|█▌                                                                              | 40/2105 [00:07<06:36,  5.21it/s]\n",
      "[I 2024-09-28 12:23:57,555] Trial 13 pruned. \n",
      "  2%|█▌                                                                              | 40/2105 [00:07<06:35,  5.22it/s]\n",
      "[I 2024-09-28 12:24:05,692] Trial 14 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:34<00:00,  5.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:34<00:00,  5.34it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:42<00:00,  5.23it/s]\n",
      "[I 2024-09-28 12:43:57,827] Trial 15 finished with value: 0.03667463510965274 and parameters: {'learning_rate': 0.00011275196976325767, 'weight_decay': 0.0007544317008571404, 'dropout_rate': 0.49255255737966247}. Best is trial 15 with value: 0.03667463510965274.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:36<00:00,  5.31it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:34<00:00,  5.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:36<00:00,  5.31it/s]\n",
      "[I 2024-09-28 13:03:46,001] Trial 16 finished with value: 0.02672645473577928 and parameters: {'learning_rate': 2.6487018908499975e-05, 'weight_decay': 0.0005487546918322648, 'dropout_rate': 0.4908808690009272}. Best is trial 16 with value: 0.02672645473577928.\n",
      "  0%|                                                                                         | 0/2105 [00:00<?, ?it/s]\n",
      "[I 2024-09-28 13:03:47,010] Trial 17 pruned. \n",
      "  1%|▊                                                                               | 20/2105 [00:03<06:51,  5.07it/s]\n",
      "[I 2024-09-28 13:03:51,710] Trial 18 pruned. \n",
      "  0%|                                                                                         | 0/2105 [00:00<?, ?it/s]\n",
      "[I 2024-09-28 13:03:52,347] Trial 19 pruned. \n",
      "  2%|█▌                                                                              | 40/2105 [00:07<06:39,  5.17it/s]\n",
      "[I 2024-09-28 13:04:00,554] Trial 20 pruned. \n",
      "  0%|                                                                                         | 0/2105 [00:00<?, ?it/s]\n",
      "[I 2024-09-28 13:04:01,481] Trial 21 pruned. \n",
      "  0%|                                                                                         | 0/2105 [00:00<?, ?it/s]\n",
      "[I 2024-09-28 13:04:02,268] Trial 22 pruned. \n",
      "  1%|▊                                                                               | 20/2105 [00:03<06:51,  5.07it/s]\n",
      "[I 2024-09-28 13:04:06,706] Trial 23 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:35<00:00,  5.32it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:35<00:00,  5.32it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:37<00:00,  5.29it/s]\n",
      "[I 2024-09-28 13:23:55,998] Trial 24 finished with value: 0.026310709884333816 and parameters: {'learning_rate': 5.750333068327132e-05, 'weight_decay': 0.0001753483985602694, 'dropout_rate': 0.5222187020192159}. Best is trial 24 with value: 0.026310709884333816.\n",
      "  3%|██▎                                                                             | 60/2105 [00:11<06:29,  5.25it/s]\n",
      "[I 2024-09-28 13:24:08,173] Trial 25 pruned. \n",
      "  0%|                                                                                         | 0/2105 [00:00<?, ?it/s]\n",
      "[I 2024-09-28 13:24:08,849] Trial 26 pruned. \n",
      "  0%|                                                                                         | 0/2105 [00:00<?, ?it/s]\n",
      "[I 2024-09-28 13:24:09,743] Trial 27 pruned. \n",
      "  1%|▊                                                                               | 20/2105 [00:03<06:48,  5.10it/s]\n",
      "[I 2024-09-28 13:24:14,142] Trial 28 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:34<00:00,  5.34it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:34<00:00,  5.34it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:33<00:00,  5.34it/s]\n",
      "[I 2024-09-28 13:43:56,896] Trial 29 finished with value: 0.44729691606408406 and parameters: {'learning_rate': 0.00037423487660806504, 'weight_decay': 0.0020020965247796226, 'dropout_rate': 0.449918238253463}. Best is trial 24 with value: 0.026310709884333816.\n",
      "  1%|▊                                                                               | 20/2105 [00:03<06:48,  5.10it/s]\n",
      "[I 2024-09-28 13:44:01,317] Trial 30 pruned. \n",
      "  0%|                                                                                         | 0/2105 [00:00<?, ?it/s]\n",
      "[I 2024-09-28 13:44:01,961] Trial 31 pruned. \n",
      "  0%|                                                                                         | 0/2105 [00:00<?, ?it/s]\n",
      "[I 2024-09-28 13:44:02,607] Trial 32 pruned. \n",
      "  1%|▊                                                                               | 20/2105 [00:03<06:49,  5.09it/s]\n",
      "[I 2024-09-28 13:44:07,333] Trial 33 pruned. \n",
      "  1%|▊                                                                               | 20/2105 [00:03<06:51,  5.07it/s]\n",
      "[I 2024-09-28 13:44:11,771] Trial 34 pruned. \n",
      "  1%|▊                                                                               | 20/2105 [00:03<06:49,  5.09it/s]\n",
      "[I 2024-09-28 13:44:16,165] Trial 35 pruned. \n",
      "  1%|▊                                                                               | 20/2105 [00:03<06:48,  5.10it/s]\n",
      "[I 2024-09-28 13:44:20,572] Trial 36 pruned. \n",
      "  0%|                                                                                         | 0/2105 [00:00<?, ?it/s]\n",
      "[I 2024-09-28 13:44:21,204] Trial 37 pruned. \n",
      "  0%|                                                                                         | 0/2105 [00:00<?, ?it/s]\n",
      "[I 2024-09-28 13:44:21,847] Trial 38 pruned. \n",
      "  0%|                                                                                         | 0/2105 [00:00<?, ?it/s]\n",
      "[I 2024-09-28 13:44:22,535] Trial 39 pruned. \n",
      "  0%|                                                                                         | 0/2105 [00:00<?, ?it/s]\n",
      "[I 2024-09-28 13:44:23,265] Trial 40 pruned. \n",
      "  0%|                                                                                         | 0/2105 [00:00<?, ?it/s]\n",
      "[I 2024-09-28 13:44:23,955] Trial 41 pruned. \n",
      "  0%|                                                                                         | 0/2105 [00:00<?, ?it/s]\n",
      "[I 2024-09-28 13:44:24,623] Trial 42 pruned. \n",
      "  0%|                                                                                         | 0/2105 [00:00<?, ?it/s]\n",
      "[I 2024-09-28 13:44:25,284] Trial 43 pruned. \n",
      "  0%|                                                                                         | 0/2105 [00:00<?, ?it/s]\n",
      "[I 2024-09-28 13:44:25,951] Trial 44 pruned. \n",
      "  1%|▊                                                                               | 20/2105 [00:03<06:49,  5.10it/s]\n",
      "[I 2024-09-28 13:44:30,337] Trial 45 pruned. \n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:36<00:00,  5.31it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:36<00:00,  5.31it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:35<00:00,  5.32it/s]\n",
      "[I 2024-09-28 14:04:19,288] Trial 46 finished with value: 0.2008344593089727 and parameters: {'learning_rate': 0.0001979768836653169, 'weight_decay': 0.005942062970369755, 'dropout_rate': 0.5339681822747883}. Best is trial 24 with value: 0.026310709884333816.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:34<00:00,  5.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:35<00:00,  5.32it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:37<00:00,  5.30it/s]\n",
      "[I 2024-09-28 14:24:08,040] Trial 47 finished with value: 0.13054388335387312 and parameters: {'learning_rate': 0.00021059189473915816, 'weight_decay': 0.0005519245517605237, 'dropout_rate': 0.5437909916706072}. Best is trial 24 with value: 0.026310709884333816.\n",
      "  1%|▊                                                                               | 20/2105 [00:03<06:55,  5.02it/s]\n",
      "[I 2024-09-28 14:24:12,688] Trial 48 pruned. \n",
      "  0%|                                                                                         | 0/2105 [00:00<?, ?it/s]\n",
      "[I 2024-09-28 14:24:13,389] Trial 49 pruned. \n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    learningrate = trial.suggest_float('learning_rate', 1e-5, 5e-3)\n",
    "    weightdecay = trial.suggest_float('weight_decay', 1e-4, 1e-2)\n",
    "    dropoutrate = trial.suggest_float('dropout_rate', 0.2, 0.7)\n",
    "\n",
    "    bert_model_tuning = SentimentClassifier(model = DistilBertModel.from_pretrained('distilbert-base-uncased') , dropout_rate = dropoutrate )\n",
    "\n",
    "    model_tuning = bert_model_tuning.to(device)\n",
    "\n",
    "    optimizer_tuning = torch.optim.AdamW(model_tuning.parameters(), lr=learningrate, weight_decay=weightdecay)\n",
    "\n",
    "\n",
    "    # Run multiple epochs and report after each epoch\n",
    "    for epoch in range(3): \n",
    "        train_acc = hypertune(\n",
    "            model_tuning,\n",
    "            optimizer_tuning,\n",
    "            train_loader,\n",
    "            criterion,\n",
    "            device,\n",
    "            trial\n",
    "        )\n",
    "\n",
    "        train_loss = 1 - train_acc  # Assuming lower accuracy means higher loss\n",
    "        \n",
    "    return train_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best_trial = study.best_trial\n",
    "learning_rate = best_trial.params['learning_rate']\n",
    "weight_decay = best_trial.params['weight_decay']\n",
    "dropout_rate = best_trial.params['dropout_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11101217-1f81-4b2e-98c3-e39b7747be96",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee14af57-d587-4619-aa43-722dd0cd6e8d",
   "metadata": {},
   "source": [
    "now we will use the best hyperparameters with our model and optimizer to proceed with the model training.\n",
    "\n",
    "we will implement **get_linear_schedule_with_warmup** scheduler , which is important for stabilizing training, especially when fine-tuning large pre-trained models like **DistilBERT**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffe5ee5c-afb2-400f-99d6-78496ea819f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_tuned = SentimentClassifier(model = DistilBertModel.from_pretrained('distilbert-base-uncased') , dropout_rate = dropout_rate )\n",
    "\n",
    "model_tuned = bert_model_tuned.to(device)\n",
    "\n",
    "optimizer_tuned = torch.optim.AdamW(model_tuned.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "total_steps = len(train_loader) * 2\n",
    "warmup_steps = int(0.1 * total_steps)  # Typically 10% of the total steps\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer_tuned,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120d5ad5-69c8-45d7-b0a8-c557bfa118d8",
   "metadata": {},
   "source": [
    "We will train with only 2 epochs , various experiments were conducted with more epochs but it ended up with the model to overfit , so it was observed that more than 2 epochs cause overfitting , this is because it's a pretrained model , 2-5 epochs will be enough , according to different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d70e0d1-6655-42e7-9915-b3471a2ab4fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Training Epoch 1 /2 ------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:36<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Train Loss: 0.2310, Train Accuracy: 0.9043, Validation Loss: 0.2822, Validation Accuracy: 0.8922\n",
      "------------------ Training Epoch 2 /2 ------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2105/2105 [06:33<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Train Loss: 0.0941, Train Accuracy: 0.9674, Validation Loss: 0.2565, Validation Accuracy: 0.8991\n",
      "Total training time: 0.22 hours\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "training_start_time = time.time()\n",
    "\n",
    "#This code is for implementing early stopping , it's useless here since we are working only with 3 epochs\n",
    "# patience = 15 # this variable is to apply early stopping technique\n",
    "# best_val_accuracy = 0.0\n",
    "# epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"------------------ Training Epoch {epoch+1} /{num_epochs} ------------------\")\n",
    "    train_accuracy, val_accuracy = model_train(model_tuned, optimizer_tuned, train_loader, val_loader, criterion, device)\n",
    "    \n",
    "    #If statement for early stopping mechanism\n",
    "    # if val_accuracy > best_val_accuracy:\n",
    "    #     best_val_accuracy = val_accuracy\n",
    "    #     epochs_without_improvement = 0\n",
    "    # else:\n",
    "    #     epochs_without_improvement += 1\n",
    "\n",
    "    # if epochs_without_improvement >= patience:\n",
    "    #     print(\"Early stopping triggered\")\n",
    "    #     break\n",
    "    \n",
    "training_end_time = time.time()\n",
    "\n",
    "# Calculate the total training time in hours\n",
    "total_training_time = (training_end_time - training_start_time) /3600\n",
    "print(f\"Total training time: {total_training_time:.2f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57009013-4e05-4c46-adf9-b12e1b80686f",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475ed7d2-4f54-439e-aae8-25718862384e",
   "metadata": {},
   "source": [
    "In this notebook, we developed an advanced sentiment analysis model utilizing the DistilBERT architecture, fine-tuning it to accurately classify sentiments from textual data. We employed get_linear_schedule_with_warmup learning rate scheduler to optimize the training process. The model was rigorously evaluated on distinct training, validation, and test sets to ensure reliable accuracy and generalization.\n",
    "\n",
    "Additionally, we prepared the model for deployment on Hugging Face (deployment is in a seperate notebook), allowing for easy testing of individual text inputs to predict sentiment classifications. \n",
    "\n",
    "This project serves as a comprehensive and practical tool for sentiment analysis, providing valuable insights for understanding public opinions and sentiments in various applications.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
